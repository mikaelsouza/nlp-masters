{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit385pyenv15b540295b4f4efd9a044469a2d20dc8",
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para geração do módulo de Dataset usando Pytorch Lightning\n",
    "\n",
    "O objetivo deste notebook é gerar um módulo de dataset com as seguintes funções:\n",
    "\n",
    "- Separe o texto em treino, teste e validação.\n",
    "- Gere os labels automaticamente a partir dos textos de dataset em `data/IWSLT/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "dataset_path = Path('../../data/IWSLT/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-f28886035e51>, line 13)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f28886035e51>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    token_list = list()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class IWSLTDataset(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_path, tokenizer):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.filenames = {\n",
    "            'train': 'train.txt',\n",
    "            'ref': 'ref.txt',\n",
    "            'dev': 'dev.txt',\n",
    "            'asr': 'asr.txt',\n",
    "        }\n",
    "    \n",
    "    def _preprocess_IWSLT(self, data, punc):\n",
    "    token_list = list()\n",
    "    punc_list = list()\n",
    "    \n",
    "    for token in data:\n",
    "        if token in punc_dict:\n",
    "            punc_list.pop()\n",
    "            punc_list.append(punc_dict[token])\n",
    "        else:\n",
    "            token_list.append(token)\n",
    "            punc_list.append(0)\n",
    "    return token_list, punc_list\n",
    "\n",
    "    def _tokens_to_sentence(self, token_list, punc_list, sentence_size=200):\n",
    "        phrases = list()\n",
    "        labels = list()\n",
    "\n",
    "        for i in range(0, len(token_list), sentence_size):\n",
    "            j = i + sentence_size if sentence_size < len(token_list) else len(token_list)\n",
    "            phrases.append(' '.join(token_list[i:j]))\n",
    "            labels.append(punc_list[i:j])\n",
    "        return phrases, labels\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # called on 1 gpu\n",
    "        pass\n",
    "\n",
    "    def setup(self):\n",
    "        # called on every GPU\n",
    "        pass\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        pass\n",
    "\n",
    "    def test_dataloader(self, ref=True):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "x = os.walk(dataset_path)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "filepath = os.path.join(dataset_path, 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data.split()\n",
    "\n",
    "punc_dict = {\n",
    "    ',COMMA':        1,\n",
    "    '.PERIOD':       2,\n",
    "    '?QUESTIONMARK': 3,\n",
    "}\n",
    "\n",
    "def preprocess_IWSLT(data: List[str], punc: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    token_list = list()\n",
    "    punc_list = list()\n",
    "    \n",
    "    for token in data:\n",
    "        if token in punc_dict:\n",
    "            punc_list.pop()\n",
    "            punc_list.append(punc_dict[token])\n",
    "        else:\n",
    "            token_list.append(token)\n",
    "            punc_list.append(0)\n",
    "    return token_list, punc_list\n",
    "\n",
    "\n",
    "token_list, punc_list = preprocess_IWSLT(data, punc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['it',\n 'can',\n 'be',\n 'a',\n 'very',\n 'complicated',\n 'thing',\n 'the',\n 'ocean',\n 'and',\n 'it',\n 'can',\n 'be',\n 'a',\n 'very',\n 'complicated',\n 'thing',\n 'what',\n 'human',\n 'health',\n 'is',\n 'and',\n 'bringing',\n 'those',\n 'two',\n 'together',\n 'might',\n 'seem',\n 'a',\n 'very',\n 'daunting',\n 'task',\n 'but',\n 'what',\n 'i',\n \"'m\",\n 'going',\n 'to',\n 'try',\n 'to',\n 'say',\n 'is',\n 'that',\n 'even',\n 'in',\n 'that',\n 'complexity',\n 'there',\n \"'s\",\n 'some']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "max_len = 50\n",
    "list(zip(token_list[:max_len], punc_list[:max_len]))\n",
    "token_list[:max_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_sentence(token_list, punc_list, sentence_size=200):\n",
    "    phrases = list()\n",
    "    labels = list()\n",
    "\n",
    "    for i in range(0, len(token_list), sentence_size):\n",
    "        j = i + sentence_size if sentence_size < len(token_list) else len(token_list)\n",
    "        phrases.append(' '.join(token_list[i:j]))\n",
    "        labels.append(punc_list[i:j])\n",
    "    return phrases, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases, labels = tokens_to_sentence(token_list, punc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(\"into the water rolf bolin who was a professor at the hopkin 's marine station where i work wrote in the 1940s that the fumes from the scum floating on the inlets of the bay were so bad they turned lead-based paints black people working in these canneries could barely stay there all day because of the smell but you know what they came out saying they say you know what you smell you smell money that pollution was money to that community and those people dealt with the pollution and absorbed it into their skin and into their bodies because they needed the money we made the ocean unhappy we made people very unhappy and we made them unhealthy the connection between ocean health and human health is actually based upon another couple simple adages and i want to call that pinch a minnow hurt a whale the pyramid of ocean life now when an ecologist looks at the ocean i have to tell you we look at the ocean in a very different way and we see different things than when a regular person looks at the ocean because when an ecologist looks at the ocean we see\",\n [0,\n  0,\n  2,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  3,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  3,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  2,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "phrases[1], labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': [[101, 2009, 2064, 2022, 1037, 2200, 8552, 2518, 1996, 4153, 1998, 2009, 2064, 2022, 1037, 2200, 8552, 2518, 2054, 2529, 2740, 2003, 1998, 5026, 2216, 2048, 2362, 2453, 4025, 1037, 2200, 4830, 16671, 2075, 4708, 2021, 2054, 1045, 1005, 1049, 2183, 2000, 3046, 2000, 2360, 2003, 2008, 2130, 1999, 2008, 11619, 2045, 1005, 1055, 2070, 3722, 6991, 2008, 1045, 2228, 2065, 2057, 3305, 2057, 2064, 2428, 2693, 2830, 1998, 2216, 3722, 6991, 2024, 1050, 1005, 1056, 2428, 6991, 2055, 1996, 3375, 2671, 1997, 2054, 1005, 1055, 2183, 2006, 2021, 2477, 2008, 2057, 2035, 3492, 2092, 2113, 1998, 1045, 1005, 1049, 2183, 2000, 2707, 2007, 2023, 2028, 2065, 23603, 9932, 1050, 1005, 1056, 3407, 9932, 1050, 1005, 1056, 6343, 3407, 2057, 2113, 2008, 2157, 2057, 1005, 2310, 5281, 2008, 1998, 2065, 2057, 2074, 2202, 2008, 1998, 2057, 3857, 2013, 2045, 2059, 2057, 2064, 2175, 2000, 1996, 2279, 3357, 2029, 2003, 2008, 2065, 1996, 4153, 9932, 1050, 1005, 1056, 3407, 9932, 1050, 1005, 1056, 6343, 3407, 2008, 1005, 1055, 1996, 4323, 1997, 2026, 2831, 1998, 2057, 1005, 2128, 2437, 1996, 4153, 3492, 12511, 1999, 1037, 2843, 1997, 2367, 3971, 2023, 2003, 1037, 2915, 1997, 2064, 27415, 5216, 1999, 4673, 2064, 27415, 5216, 2012, 1996, 2051, 2018, 1996, 5221, 3919, 24549, 3169, 2006, 1996, 2225, 3023, 2057, 17835, 8216, 8310, 1997, 10796, 2046, 1996, 2250, 1998, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(phrases[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}