{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit385pyenv15b540295b4f4efd9a044469a2d20dc8",
   "display_name": "Python 3.8.5 64-bit ('3.8.5': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para geração do módulo de Dataset usando Pytorch Lightning\n",
    "\n",
    "O objetivo deste notebook é gerar um módulo de dataset com as seguintes funções:\n",
    "\n",
    "- Separe o texto em treino, teste e validação.\n",
    "- Gere os labels automaticamente a partir dos textos de dataset em `data/IWSLT/raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dataset_path = Path('../../data/IWSLT/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "x = os.walk(dataset_path)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "filepath = os.path.join(dataset_path, 'train.txt')\n",
    "\n",
    "punc_dict = {\n",
    "    ',COMMA':        1,\n",
    "    '.PERIOD':       2,\n",
    "    '?QUESTIONMARK': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IWSLTDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len, punc_dict, tok_max_len=278):\n",
    "\n",
    "        self.path = path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.punc_dict = punc_dict\n",
    "        self.tok_max_len = tok_max_len\n",
    "\n",
    "        data = self._load_data(path)\n",
    "        token_list, punc_list = self._preprocess_IWSLT(data)\n",
    "        self.data, self.labels = self._tokens_to_sentence(token_list, punc_list, max_len)\n",
    "\n",
    "    def _load_data(self, path):\n",
    "        with open(path) as f:\n",
    "            data = f.read()\n",
    "            data = data.split()\n",
    "        return data\n",
    "\n",
    "    def _preprocess_IWSLT(self, data):\n",
    "        token_list = list()\n",
    "        punc_list = list()\n",
    "        \n",
    "        for token in data:\n",
    "            if token in punc_dict:\n",
    "                punc_list.pop()\n",
    "                punc_list.append(self.punc_dict[token])\n",
    "            else:\n",
    "                token_list.append(token)\n",
    "                punc_list.append(0)\n",
    "        return token_list, punc_list\n",
    "\n",
    "    def _tokens_to_sentence(self, token_list, punc_list, max_len):\n",
    "        phrases = list()\n",
    "        labels = list()\n",
    "\n",
    "        for i in range(0, len(token_list), max_len):\n",
    "            j = i + max_len if max_len < len(token_list) else len(token_list)\n",
    "            phrases.append(' '.join(token_list[i:j]))\n",
    "            labels.append(punc_list[i:j])\n",
    "        return phrases, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = tokenizer.encode_plus(\n",
    "            self.data[idx],\n",
    "            max_length=self.tok_max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        target = torch.LongTensor(self.labels[idx])\n",
    "    \n",
    "        return {\n",
    "            'sentence': self.data[idx],\n",
    "            'input_ids': data['input_ids'],\n",
    "            'attention_mask': data['attention_mask'],\n",
    "            'target': target,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = IWSLTDataset(path='../../data/IWSLT/raw/train.txt',\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_len=200,\n",
    "                    punc_dict=punc_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'sentence': \"not the i have a plan speech listen to politicians now with their comprehensive 12-point plans they 're not inspiring anybody because there are leaders and there are those who lead leaders hold a position of power or authority but those who lead inspire us whether they 're individuals or organizations we follow those who lead not because we have to but because we want to we follow those who lead not for them but for ourselves and it 's those who start with why that have the ability to inspire those around them or find others who inspire them thank you very much\",\n 'input_ids': tensor([[  101,  2025,  1996,  1045,  2031,  1037,  2933,  4613,  4952,  2000,\n           8801,  2085,  2007,  2037,  7721,  2260,  1011,  2391,  3488,  2027,\n           1005,  2128,  2025, 18988, 10334,  2138,  2045,  2024,  4177,  1998,\n           2045,  2024,  2216,  2040,  2599,  4177,  2907,  1037,  2597,  1997,\n           2373,  2030,  3691,  2021,  2216,  2040,  2599, 18708,  2149,  3251,\n           2027,  1005,  2128,  3633,  2030,  4411,  2057,  3582,  2216,  2040,\n           2599,  2025,  2138,  2057,  2031,  2000,  2021,  2138,  2057,  2215,\n           2000,  2057,  3582,  2216,  2040,  2599,  2025,  2005,  2068,  2021,\n           2005,  9731,  1998,  2009,  1005,  1055,  2216,  2040,  2707,  2007,\n           2339,  2008,  2031,  1996,  3754,  2000, 18708,  2216,  2105,  2068,\n           2030,  2424,  2500,  2040, 18708,  2068,  4067,  2017,  2200,  2172,\n            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'target': tensor([0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0,\n         0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 2, 0, 0, 0, 2])}"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "ds[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IWSLTDataset2(pl.LightningDataModule):\n",
    "    def __init__(self, dataset_path, tokenizer, ref=True):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ref = ref\n",
    "        self.data = None\n",
    "        self.dataset_path = dataset_path\n",
    "        self.filenames = {\n",
    "            'train': 'train.txt',\n",
    "            'ref': 'ref.txt',\n",
    "            'dev': 'dev.txt',\n",
    "            'asr': 'asr.txt',\n",
    "        }\n",
    "        self.punc_dict = {\n",
    "            ',COMMA':        1,\n",
    "            '.PERIOD':       2,\n",
    "            '?QUESTIONMARK': 3,\n",
    "        }\n",
    "\n",
    "    def _load_data(self, split):\n",
    "        import os\n",
    "        split = self.filenames[split]\n",
    "        datapath = os.path.join(self.dataset_path, split)\n",
    "        with open(datapath) as f:\n",
    "            self.data = f.read()\n",
    "            self.data = self.data.split()\n",
    "\n",
    "    def _preprocess_IWSLT(self):\n",
    "        token_list = list()\n",
    "        punc_list = list()\n",
    "        \n",
    "        for token in self.data:\n",
    "            if token in self.punc_dict:\n",
    "                punc_list.pop()\n",
    "                punc_list.append(self.punc_dict[token])\n",
    "            else:\n",
    "                token_list.append(token)\n",
    "                punc_list.append(0)\n",
    "        return token_list, punc_list\n",
    "\n",
    "    def _tokens_to_sentence(self, token_list, punc_list, sentence_size):\n",
    "        phrases = list()\n",
    "        labels = list()\n",
    "\n",
    "        for i in range(0, len(token_list), sentence_size):\n",
    "            j = i + sentence_size if sentence_size < len(token_list) else len(token_list)\n",
    "            phrases.append(' '.join(token_list[i:j]))\n",
    "            labels.append(punc_list[i:j])\n",
    "        return phrases, labels\n",
    "\n",
    "    def _get_data(self, sentence_size, split='train'):\n",
    "        self._load_data(split)\n",
    "        token_list, punc_list = self._preprocess_IWSLT()\n",
    "        phrases, labels = self._tokens_to_sentence(token_list, punc_list, sentence_size)\n",
    "        return phrases, labels\n",
    "\n",
    "    def prepare_data(self, sentence_size=200):\n",
    "        self.train_X, self.train_y = self._get_data(split='train', sentence_size=sentence_size)\n",
    "        self.dev_X, self.dev_y = self._get_data(split='dev', sentence_size=sentence_size)\n",
    "        self.test_X, self.test_y = self._get_data(split='ref', sentence_size=sentence_size)\n",
    "        \n",
    "        self.train_X = self.train_X[:-1]\n",
    "        self.dev_X = self.dev_X[:-1]\n",
    "        self.test_X = self.test_X[:-1]\n",
    "\n",
    "        self.train_y = self.train_y[:-1]\n",
    "        self.dev_y = self.dev_y[:-1]\n",
    "        self.test_y = self.test_y[:-1]\n",
    "\n",
    "        self.train_X = self.tokenizer.batch_encode_plus(self.train_X, pad_to_max_length=True,)\n",
    "        self.dev_X = self.tokenizer.batch_encode_plus(self.dev_X, pad_to_max_length=True,)\n",
    "        self.test_X = self.tokenizer.batch_encode_plus(self.test_X, pad_to_max_length=True,)\n",
    "\n",
    "        self.train_y = torch.LongTensor(self.train_y)\n",
    "        self.dev_y = torch.LongTensor(self.dev_y)\n",
    "        self.test_y = torch.LongTensor(self.test_y)\n",
    "\n",
    "    def setup(self):\n",
    "        pass\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_X, batch_size=1)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_X, batch_size=1)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_X, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = IWSLTDataset(dataset_path, tokenizer)\n",
    "ds.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "276"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "len(ds.train_X['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "GPU available: True, used: False\nTPU available: False, using: 0 TPU cores\n"
    }
   ],
   "source": [
    "trainer = pl.Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath) as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data.split()\n",
    "\n",
    "punc_dict = {\n",
    "    ',COMMA':        1,\n",
    "    '.PERIOD':       2,\n",
    "    '?QUESTIONMARK': 3,\n",
    "}\n",
    "\n",
    "def preprocess_IWSLT(data: List[str], punc: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    token_list = list()\n",
    "    punc_list = list()\n",
    "    \n",
    "    for token in data:\n",
    "        if token in punc_dict:\n",
    "            punc_list.pop()\n",
    "            punc_list.append(punc_dict[token])\n",
    "        else:\n",
    "            token_list.append(token)\n",
    "            punc_list.append(0)\n",
    "    return token_list, punc_list\n",
    "\n",
    "\n",
    "token_list, punc_list = preprocess_IWSLT(data, punc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['it',\n 'can',\n 'be',\n 'a',\n 'very',\n 'complicated',\n 'thing',\n 'the',\n 'ocean',\n 'and',\n 'it',\n 'can',\n 'be',\n 'a',\n 'very',\n 'complicated',\n 'thing',\n 'what',\n 'human',\n 'health',\n 'is',\n 'and',\n 'bringing',\n 'those',\n 'two',\n 'together',\n 'might',\n 'seem',\n 'a',\n 'very',\n 'daunting',\n 'task',\n 'but',\n 'what',\n 'i',\n \"'m\",\n 'going',\n 'to',\n 'try',\n 'to',\n 'say',\n 'is',\n 'that',\n 'even',\n 'in',\n 'that',\n 'complexity',\n 'there',\n \"'s\",\n 'some']"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "max_len = 50\n",
    "list(zip(token_list[:max_len], punc_list[:max_len]))\n",
    "token_list[:max_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_sentence(token_list, punc_list, sentence_size=200):\n",
    "    phrases = list()\n",
    "    labels = list()\n",
    "\n",
    "    for i in range(0, len(token_list), sentence_size):\n",
    "        j = i + sentence_size if sentence_size < len(token_list) else len(token_list)\n",
    "        phrases.append(' '.join(token_list[i:j]))\n",
    "        labels.append(punc_list[i:j])\n",
    "    return phrases, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases, labels = tokens_to_sentence(token_list, punc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(\"into the water rolf bolin who was a professor at the hopkin 's marine station where i work wrote in the 1940s that the fumes from the scum floating on the inlets of the bay were so bad they turned lead-based paints black people working in these canneries could barely stay there all day because of the smell but you know what they came out saying they say you know what you smell you smell money that pollution was money to that community and those people dealt with the pollution and absorbed it into their skin and into their bodies because they needed the money we made the ocean unhappy we made people very unhappy and we made them unhealthy the connection between ocean health and human health is actually based upon another couple simple adages and i want to call that pinch a minnow hurt a whale the pyramid of ocean life now when an ecologist looks at the ocean i have to tell you we look at the ocean in a very different way and we see different things than when a regular person looks at the ocean because when an ecologist looks at the ocean we see\",\n [0,\n  0,\n  2,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  3,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  3,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  2,\n  0,\n  0,\n  0,\n  0,\n  2,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  1,\n  0,\n  0])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "phrases[1], labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': [[101, 2009, 2064, 2022, 1037, 2200, 8552, 2518, 1996, 4153, 1998, 2009, 2064, 2022, 1037, 2200, 8552, 2518, 2054, 2529, 2740, 2003, 1998, 5026, 2216, 2048, 2362, 2453, 4025, 1037, 2200, 4830, 16671, 2075, 4708, 2021, 2054, 1045, 1005, 1049, 2183, 2000, 3046, 2000, 2360, 2003, 2008, 2130, 1999, 2008, 11619, 2045, 1005, 1055, 2070, 3722, 6991, 2008, 1045, 2228, 2065, 2057, 3305, 2057, 2064, 2428, 2693, 2830, 1998, 2216, 3722, 6991, 2024, 1050, 1005, 1056, 2428, 6991, 2055, 1996, 3375, 2671, 1997, 2054, 1005, 1055, 2183, 2006, 2021, 2477, 2008, 2057, 2035, 3492, 2092, 2113, 1998, 1045, 1005, 1049, 2183, 2000, 2707, 2007, 2023, 2028, 2065, 23603, 9932, 1050, 1005, 1056, 3407, 9932, 1050, 1005, 1056, 6343, 3407, 2057, 2113, 2008, 2157, 2057, 1005, 2310, 5281, 2008, 1998, 2065, 2057, 2074, 2202, 2008, 1998, 2057, 3857, 2013, 2045, 2059, 2057, 2064, 2175, 2000, 1996, 2279, 3357, 2029, 2003, 2008, 2065, 1996, 4153, 9932, 1050, 1005, 1056, 3407, 9932, 1050, 1005, 1056, 6343, 3407, 2008, 1005, 1055, 1996, 4323, 1997, 2026, 2831, 1998, 2057, 1005, 2128, 2437, 1996, 4153, 3492, 12511, 1999, 1037, 2843, 1997, 2367, 3971, 2023, 2003, 1037, 2915, 1997, 2064, 27415, 5216, 1999, 4673, 2064, 27415, 5216, 2012, 1996, 2051, 2018, 1996, 5221, 3919, 24549, 3169, 2006, 1996, 2225, 3023, 2057, 17835, 8216, 8310, 1997, 10796, 2046, 1996, 2250, 1998, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(phrases[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}