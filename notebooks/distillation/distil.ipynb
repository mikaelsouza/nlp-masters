{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste de distilação de modelo usando BERT e BiLSTM no dataset SST-2.\n",
    "\n",
    "Neste notebook exploraremos a distilação de conhecimento utilizando o modelo pré-treinado BERT como professor e treinando uma BiLSTM como aluna.\n",
    "\n",
    "### Carregando Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentenças de Treino: \n",
      " [\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", \"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\", 'Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .', \"You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\", 'Yet the act is still charming here .', \"Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\", 'Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .', 'Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness .', \"a screenplay more ingeniously constructed than `` Memento ''\", \"`` Extreme Ops '' exceeds expectations .\"]\n",
      "Sentenças de Teste: \n",
      " ['Effective but too-tepid biopic', 'If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .', \"Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\", 'The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', 'Offers that rare combination of entertainment and education .', 'Perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .', \"Steers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it .\", 'But he somehow pulls it off .', 'Take Care of My Cat offers a refreshingly different slice of Asian cinema .', 'This is a film well worth seeing , talking and singing heads and all .']\n",
      "Sentenças de Dev: \n",
      " [\"It 's a lovely film with lovely performances by Buy and Accorsi .\", 'No one goes unindicted here , which is probably for the best .', \"And if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\", 'A warm , funny , engaging film .', 'Uses sharp humor and insight into human nature to examine class conflict , adolescent yearning , the roots of friendship and sexual identity .', 'Half Submarine flick , Half Ghost Story , All in one criminally neglected film', 'Entertains by providing good , lively company .', \"Dazzles with its fully-written characters , its determined stylishness -LRB- which always relates to characters and story -RRB- and Johnny Dankworth 's best soundtrack in years .\", 'Visually imaginative , thematically instructive and thoroughly delightful , it takes us on a roller-coaster ride from innocence to experience without even a hint of that typical kiddie-flick sentimentality .', \"Nothing 's at stake , just a twisty double-cross you can smell a mile away -- still , the derivative Nine Queens is lots of fun .\"]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "def load_sentences(data_folder, file):\n",
    "    path = join(data_folder, file)\n",
    "    sentences = open(path).readlines()\n",
    "    sentences = list(map(lambda x: x.strip(), sentences))\n",
    "    return sentences\n",
    "\n",
    "data_folder = Path('../../data/STT2/')\n",
    "\n",
    "train_file = Path('train.txt')\n",
    "test_file = Path('test.txt')\n",
    "dev_file = Path('dev.txt')\n",
    "\n",
    "train_sentences = load_sentences(data_folder, train_file)\n",
    "test_sentences = load_sentences(data_folder, test_file)\n",
    "dev_sentences = load_sentences(data_folder, dev_file)\n",
    "\n",
    "print(\"Sentenças de Treino: \\n\", train_sentences[:10])\n",
    "print(\"Sentenças de Teste: \\n\", test_sentences[:10])\n",
    "print(\"Sentenças de Dev: \\n\", dev_sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "embedding = bert_model.bert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formato do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.bert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novo Modelo BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=150,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=300,\n",
    "            out_features=200,\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            in_features=200,\n",
    "            out_features=2,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        _, (last_state, _) = self.bilstm(x)\n",
    "        last_state = last_state.view(x.size(0), -1)\n",
    "        dense_state = nn.functional.relu(self.dense(last_state))\n",
    "        logits = self.output(dense_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class STT2Dataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, embedding, bert, max_length=50):\n",
    "        self.data = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert = bert     \n",
    "        self.max_length = max_length\n",
    "        self.embedding = embedding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inputs = tokenizer(self.data[idx],\n",
    "                          return_tensors='pt',\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=self.max_length)\n",
    "        \n",
    "        self.embedding.eval()\n",
    "        self.bert.eval()\n",
    "        with torch.no_grad():\n",
    "            bert_logits = self.bert(**inputs)[0]\n",
    "            inputs = self.embedding(inputs['input_ids'])\n",
    "        \n",
    "        return inputs.squeeze(0), bert_logits.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss: tensor(7.8126)\n",
      "Epoch:  1 Loss: tensor(3.8722)\n",
      "Epoch:  2 Loss: tensor(3.7157)\n",
      "Epoch:  3 Loss: tensor(3.5315)\n",
      "Epoch:  4 Loss: tensor(3.4798)\n"
     ]
    }
   ],
   "source": [
    "bilstm_model = BiLSTM()\n",
    "optim = torch.optim.Adam(bilstm_model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "examples = train_sentences[:500]\n",
    "train_dataset = STT2Dataset(examples, tokenizer, embedding, bert_model)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=8)\n",
    "\n",
    "for e in range(5):\n",
    "    e_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        x, y = batch\n",
    "        logits = bilstm_model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        e_loss += loss.data\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    e_loss = e_loss / len(train_dataloader)\n",
    "    print(\"Epoch: \", e, \"Loss:\", e_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
