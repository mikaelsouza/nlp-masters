{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste de distilação de modelo usando BERT e BiLSTM no dataset SST-2.\n",
    "\n",
    "Neste notebook exploraremos a distilação de conhecimento utilizando o modelo pré-treinado BERT como professor e treinando uma BiLSTM como aluna.\n",
    "\n",
    "### Carregando Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Sentenças de Treino: \n [&quot;The Rock is destined to be the 21st Century &#39;s new `` Conan &#39;&#39; and that he &#39;s going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .&quot;, &quot;The gorgeously elaborate continuation of `` The Lord of the Rings &#39;&#39; trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson &#39;s expanded vision of J.R.R. Tolkien &#39;s Middle-earth .&quot;, &#39;Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .&#39;, &quot;You &#39;d think by now America would have had enough of plucky British eccentrics with hearts of gold .&quot;, &#39;Yet the act is still charming here .&#39;, &quot;Whether or not you &#39;re enlightened by any of Derrida &#39;s lectures on `` the other &#39;&#39; and `` the self , &#39;&#39; Derrida is an undeniably fascinating and playful fellow .&quot;, &#39;Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .&#39;, &#39;Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness .&#39;, &quot;a screenplay more ingeniously constructed than `` Memento &#39;&#39;&quot;, &quot;`` Extreme Ops &#39;&#39; exceeds expectations .&quot;]\nSentenças de Teste: \n [&#39;Effective but too-tepid biopic&#39;, &#39;If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .&#39;, &quot;Emerges as something rare , an issue movie that &#39;s so honest and keenly observed that it does n&#39;t feel like one .&quot;, &#39;The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .&#39;, &#39;Offers that rare combination of entertainment and education .&#39;, &#39;Perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .&#39;, &quot;Steers turns in a snappy screenplay that curls at the edges ; it &#39;s so clever you want to hate it .&quot;, &#39;But he somehow pulls it off .&#39;, &#39;Take Care of My Cat offers a refreshingly different slice of Asian cinema .&#39;, &#39;This is a film well worth seeing , talking and singing heads and all .&#39;]\nSentenças de Dev: \n [&quot;It &#39;s a lovely film with lovely performances by Buy and Accorsi .&quot;, &#39;No one goes unindicted here , which is probably for the best .&#39;, &quot;And if you &#39;re not nearly moved to tears by a couple of scenes , you &#39;ve got ice water in your veins .&quot;, &#39;A warm , funny , engaging film .&#39;, &#39;Uses sharp humor and insight into human nature to examine class conflict , adolescent yearning , the roots of friendship and sexual identity .&#39;, &#39;Half Submarine flick , Half Ghost Story , All in one criminally neglected film&#39;, &#39;Entertains by providing good , lively company .&#39;, &quot;Dazzles with its fully-written characters , its determined stylishness -LRB- which always relates to characters and story -RRB- and Johnny Dankworth &#39;s best soundtrack in years .&quot;, &#39;Visually imaginative , thematically instructive and thoroughly delightful , it takes us on a roller-coaster ride from innocence to experience without even a hint of that typical kiddie-flick sentimentality .&#39;, &quot;Nothing &#39;s at stake , just a twisty double-cross you can smell a mile away -- still , the derivative Nine Queens is lots of fun .&quot;]\nTamanho de Treino: \n 8544\nTamanho de Teste: \n 2210\nTamanho de Dev: \n 1101\n"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "def load_sentences(data_folder, file):\n",
    "    path = join(data_folder, file)\n",
    "    sentences = open(path).readlines()\n",
    "    sentences = list(map(lambda x: x.strip(), sentences))\n",
    "    return sentences\n",
    "\n",
    "data_folder = Path('../../data/STT2/')\n",
    "\n",
    "train_file = Path('train.txt')\n",
    "test_file = Path('test.txt')\n",
    "dev_file = Path('dev.txt')\n",
    "\n",
    "train_sentences = load_sentences(data_folder, train_file)\n",
    "test_sentences = load_sentences(data_folder, test_file)\n",
    "dev_sentences = load_sentences(data_folder, dev_file)\n",
    "\n",
    "print(\"Sentenças de Treino: \\n\", train_sentences[:10])\n",
    "print(\"Sentenças de Teste: \\n\", test_sentences[:10])\n",
    "print(\"Sentenças de Dev: \\n\", dev_sentences[:10])\n",
    "\n",
    "print(\"Tamanho de Treino: \\n\", len(train_sentences))\n",
    "print(\"Tamanho de Teste: \\n\", len(test_sentences))\n",
    "print(\"Tamanho de Dev: \\n\", len(dev_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "embedding = bert_model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 25\n",
    "\n",
    "examples = train_sentences\n",
    "inputs = tokenizer(examples,\n",
    "                          return_tensors='pt',\n",
    "                          padding='max_length',\n",
    "                          truncation=True,\n",
    "                          max_length=max_length)\n",
    "\n",
    "embedding.eval()\n",
    "bert_model.eval()\n",
    "with torch.no_grad():\n",
    "    bert_logits = bert_model(**inputs)[0]\n",
    "    inputs = embedding(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formato do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertEmbeddings(\n  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n  (position_embeddings): Embedding(512, 768)\n  (token_type_embeddings): Embedding(2, 768)\n  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "bert_model.bert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novo Modelo BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class BiLSTM(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=150,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=300,\n",
    "            out_features=200,\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            in_features=200,\n",
    "            out_features=2,\n",
    "        )\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (last_state, _) = self.bilstm(x)\n",
    "        last_state = last_state.view(x.size(0), -1)\n",
    "        dense_state = F.dropout(nn.functional.relu(self.dense(last_state)), 0.5)\n",
    "        logits = self.output(dense_state)\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class STT2Dataset(Dataset):\n",
    "    def __init__(self, inputs, bert_logits):\n",
    "        self.inputs = inputs\n",
    "        self.bert_logits = bert_logits\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.bert_logits[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "GPU available: False, used: False\nTPU available: False, using: 0 TPU cores\n\n  | Name   | Type   | Params\n----------------------------------\n0 | bilstm | LSTM   | 1 M   \n1 | dense  | Linear | 60 K  \n2 | output | Linear | 402   \n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=&#39;Finding best initial lr&#39;), FloatProgress(value=0.0), HTML(value=&#39;&#39;)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ea1ca1207d2475484abfc27a2eb1c12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Saving latest checkpoint..\nLR finder stopped early due to diverging loss.\nLearning rate set to 2.7542287033381663e-07\n\n  | Name   | Type   | Params\n----------------------------------\n0 | bilstm | LSTM   | 1 M   \n1 | dense  | Linear | 60 K  \n2 | output | Linear | 402   \n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=&#39;Training&#39;), FloatProgress(value=1.0, bar_style=&#39;info&#39;, layout=Layout(flex=&#39;2&#39;), max…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "582952c0b70a4a58a8d68de9c49b449f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "bilstm_model = BiLSTM()\n",
    "bilstm_model.lr = 0\n",
    "\n",
    "train_dataset = STT2Dataset(inputs, bert_logits)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=12)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, accumulate_grad_batches=len(train_dataloader), auto_lr_find=True)\n",
    "trainer.fit(bilstm_model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('.env': venv)",
   "display_name": "Python 3.6.9 64-bit ('.env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "fa2a498ff3c55499b6df688db1fd2ed0f26381507c26cda639da92624b3b6bce"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}