{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste de distilação de modelo usando BERT e BiLSTM no dataset SST-2.\n",
    "\n",
    "Neste notebook exploraremos a distilação de conhecimento utilizando o modelo pré-treinado BERT como professor e treinando uma BiLSTM como aluna.\n",
    "\n",
    "### Carregando Base de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentenças de Treino: \n [\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\", \"The gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\", 'Singer\\\\/composer Bryan Adams contributes a slew of songs -- a few potential hits , a few more simply intrusive to the story -- but the whole package certainly captures the intended , er , spirit of the piece .', \"You 'd think by now America would have had enough of plucky British eccentrics with hearts of gold .\", 'Yet the act is still charming here .', \"Whether or not you 're enlightened by any of Derrida 's lectures on `` the other '' and `` the self , '' Derrida is an undeniably fascinating and playful fellow .\", 'Just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .', 'Part of the charm of Satin Rouge is that it avoids the obvious with humour and lightness .', \"a screenplay more ingeniously constructed than `` Memento ''\", \"`` Extreme Ops '' exceeds expectations .\"]\nSentenças de Teste: \n ['Effective but too-tepid biopic', 'If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .', \"Emerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\", 'The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .', 'Offers that rare combination of entertainment and education .', 'Perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .', \"Steers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it .\", 'But he somehow pulls it off .', 'Take Care of My Cat offers a refreshingly different slice of Asian cinema .', 'This is a film well worth seeing , talking and singing heads and all .']\nSentenças de Dev: \n [\"It 's a lovely film with lovely performances by Buy and Accorsi .\", 'No one goes unindicted here , which is probably for the best .', \"And if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\", 'A warm , funny , engaging film .', 'Uses sharp humor and insight into human nature to examine class conflict , adolescent yearning , the roots of friendship and sexual identity .', 'Half Submarine flick , Half Ghost Story , All in one criminally neglected film', 'Entertains by providing good , lively company .', \"Dazzles with its fully-written characters , its determined stylishness -LRB- which always relates to characters and story -RRB- and Johnny Dankworth 's best soundtrack in years .\", 'Visually imaginative , thematically instructive and thoroughly delightful , it takes us on a roller-coaster ride from innocence to experience without even a hint of that typical kiddie-flick sentimentality .', \"Nothing 's at stake , just a twisty double-cross you can smell a mile away -- still , the derivative Nine Queens is lots of fun .\"]\nTamanho de Treino: \n 8544\nTamanho de Teste: \n 2210\nTamanho de Dev: \n 1101\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "def load_sentences(data_folder, file):\n",
    "    path = join(data_folder, file)\n",
    "    sentences = open(path).readlines()\n",
    "    sentences = list(map(lambda x: x.strip(), sentences))\n",
    "    return sentences\n",
    "\n",
    "data_folder = Path('../../data/STT2/')\n",
    "\n",
    "train_file = Path('train.txt')\n",
    "test_file = Path('test.txt')\n",
    "dev_file = Path('dev.txt')\n",
    "\n",
    "train_sentences = load_sentences(data_folder, train_file)\n",
    "test_sentences = load_sentences(data_folder, test_file)\n",
    "dev_sentences = load_sentences(data_folder, dev_file)\n",
    "\n",
    "print(\"Sentenças de Treino: \\n\", train_sentences[:10])\n",
    "print(\"Sentenças de Teste: \\n\", test_sentences[:10])\n",
    "print(\"Sentenças de Dev: \\n\", dev_sentences[:10])\n",
    "\n",
    "print(\"Tamanho de Treino: \\n\", len(train_sentences))\n",
    "print(\"Tamanho de Teste: \\n\", len(test_sentences))\n",
    "print(\"Tamanho de Dev: \\n\", len(dev_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "embedding = bert_model.bert.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pasta de Tensores Encontrada. Carregando Tensores...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tensors_folder = Path('tensors')\n",
    "tensors_path = os.path.join(data_folder, tensors_folder)\n",
    "\n",
    "if not os.path.isdir(tensors_path):\n",
    "    print(\"Pasta de Tensores Não Encontrada.\")\n",
    "    os.mkdir(tensors_path)\n",
    "    print(\"Gerando Tensores. Espere.\")\n",
    "\n",
    "    max_length = 35\n",
    "    print(\"Gerando Tokens...\")\n",
    "    examples = train_sentences\n",
    "    inputs = tokenizer(examples,\n",
    "                            return_tensors='pt',\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            max_length=max_length)\n",
    "\n",
    "    dev_inputs = tokenizer(dev_sentences,\n",
    "                            return_tensors='pt',\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            max_length=max_length)\n",
    "\n",
    "    test_inputs = tokenizer(test_sentences,\n",
    "                            return_tensors='pt',\n",
    "                            padding='max_length',\n",
    "                            truncation=True,\n",
    "                            max_length=max_length)\n",
    "\n",
    "    embedding.eval()\n",
    "    bert_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Gerando Tensores de Treino...\")\n",
    "        bert_logits = bert_model(**inputs)[0]\n",
    "        inputs = embedding(inputs['input_ids'])\n",
    "\n",
    "        torch.save(bert_logits, os.path.join(tensors_path, Path(\"train_logits.pt\")))\n",
    "        torch.save(inputs, os.path.join(tensors_path, Path(\"train_inputs.pt\")))\n",
    "\n",
    "        print(\"Gerando Tensores de Validação...\")\n",
    "        dev_bert_logits = bert_model(**dev_inputs)[0]\n",
    "        dev_inputs = embedding(dev_inputs['input_ids'])\n",
    "\n",
    "        torch.save(dev_bert_logits, os.path.join(tensors_path, Path(\"dev_logits.pt\")))\n",
    "        torch.save(dev_inputs, os.path.join(tensors_path, Path(\"dev_inputs.pt\")))\n",
    "\n",
    "        print(\"Gerando Tensores de Teste...\")\n",
    "        test_bert_logits = bert_model(**test_inputs)[0]\n",
    "        test_inputs = embedding(test_inputs['input_ids'])\n",
    "\n",
    "        torch.save(test_bert_logits, os.path.join(tensors_path, Path(\"test_logits.pt\")))\n",
    "        torch.save(test_inputs, os.path.join(tensors_path, Path(\"test_inputs.pt\")))\n",
    "\n",
    "else:\n",
    "    print(\"Pasta de Tensores Encontrada. Carregando Tensores...\")\n",
    "    bert_logits = torch.load(os.path.join(tensors_path, Path(\"train_logits.pt\")))\n",
    "    inputs = torch.load(os.path.join(tensors_path, Path(\"train_inputs.pt\")))\n",
    "\n",
    "    dev_bert_logits = torch.load(os.path.join(tensors_path, Path(\"dev_logits.pt\")))\n",
    "    dev_inputs = torch.load(os.path.join(tensors_path, Path(\"dev_inputs.pt\")))\n",
    "        \n",
    "    test_bert_logits = torch.load(os.path.join(tensors_path, Path(\"test_logits.pt\")))\n",
    "    test_inputs = torch.load(os.path.join(tensors_path, Path(\"test_inputs.pt\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formato do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "bert_model.bert.embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Novo Modelo BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from ranger import Ranger \n",
    "\n",
    "class BiLSTM(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=150,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=300,\n",
    "            out_features=200,\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            in_features=200,\n",
    "            out_features=2,\n",
    "        )\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Ranger(self.parameters())\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (last_state, _) = self.bilstm(x)\n",
    "        last_state = last_state.view(x.size(0), -1)\n",
    "        dropped_last_state = F.dropout(last_state, 0.2)\n",
    "        dense_state = nn.functional.relu(self.dense(dropped_last_state))\n",
    "        dropped_dense_state = F.dropout(dense_state, 0.2)\n",
    "        logits = self.output(dropped_dense_state)\n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log('train_loss', loss, prog_bar=True)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss, prog_bar=True)\n",
    "        return result\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        result = pl.EvalResult()\n",
    "        result.log('test_loss', loss, prog_bar=True)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class STT2Dataset(Dataset):\n",
    "    def __init__(self, inputs, bert_logits):\n",
    "        self.inputs = inputs\n",
    "        self.bert_logits = bert_logits\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.bert_logits[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'BiLSTM' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-03bbcf276a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbilstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTT2Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BiLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "bilstm_model = BiLSTM()\n",
    "\n",
    "train_dataset = STT2Dataset(inputs, bert_logits)\n",
    "dev_dataset = STT2Dataset(dev_inputs, dev_bert_logits)\n",
    "test_dataset = STT2Dataset(test_inputs, test_bert_logits)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    gpus=[0],\n",
    ")\n",
    "trainer.fit(bilstm_model, train_dataloader, dev_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4f26d5cbc56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.test(test_dataloaders=test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38564bit3856820c561336c419cb4f7d89d1d885626",
   "display_name": "Python 3.8.5 64-bit ('3.8.5')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}