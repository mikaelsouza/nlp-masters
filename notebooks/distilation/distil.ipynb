{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit385d46e34b1a57242a7b4ba2382beefc69f",
   "display_name": "Python 3.8.5 64-bit ('3.8.5')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Teste de distilação de modelo usando BERT e BiLSTM no dataset SST-2.\n",
    "\n",
    "Neste notebook exploraremos a distilação de conhecimento utilizando o modelo pré-treinado BERT como professor e treinando uma BiLSTM como aluna.\n",
    "\n",
    "### Carregando Base de Dados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&quot;The Rock is destined to be the 21st Century &#39;s new `` Conan &#39;&#39; and that he &#39;s going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .&quot;, &quot;The gorgeously elaborate continuation of `` The Lord of the Rings &#39;&#39; trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson &#39;s expanded vision of J.R.R. Tolkien &#39;s Middle-earth .&quot;, &#39;Effective but too-tepid biopic&#39;, &#39;If you sometimes like to go to the movies to have fun , Wasabi is a good place to start .&#39;, &quot;Emerges as something rare , an issue movie that &#39;s so honest and keenly observed that it does n&#39;t feel like one .&quot;, &#39;The film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .&#39;, &#39;Offers that rare combination of entertainment and education .&#39;, &#39;Perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .&#39;, &quot;Steers turns in a snappy screenplay that curls at the edges ; it &#39;s so clever you want to hate it .&quot;, &#39;But he somehow pulls it off .&#39;]\n"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from os.path import join\n",
    "\n",
    "data_folder = Path('../../data/STT2/')\n",
    "sentences_file = Path('sentences.txt')\n",
    "sentences_path = join(data_folder, sentences_file)\n",
    "\n",
    "sentences = open(sentences_path).readlines()[1:]\n",
    "sentences = list(map(lambda x: x.strip(), sentences))\n",
    "sentences = list(map(lambda x: x.split('\\t')[1], sentences))\n",
    "\n",
    "print(sentences[:10])"
   ]
  },
  {
   "source": [
    "### Carregamento do Modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "embedding = bert_model.bert.embeddings"
   ]
  },
  {
   "source": [
    "### Formato do Modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertEmbeddings(\n  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n  (position_embeddings): Embedding(512, 768)\n  (token_type_embeddings): Embedding(2, 768)\n  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "bert_model.bert.embeddings"
   ]
  },
  {
   "source": [
    "### Predição"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = sentences[:20]\n",
    "inputs = tokenizer(examples, return_tensors='pt', padding=True)\n",
    "\n",
    "bert_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_logits = bert_model(**inputs)[0]\n",
    "\n",
    "embedding.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_representations = embedding(inputs['input_ids'])"
   ]
  },
  {
   "source": [
    "### Novo Modelo BiLSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=150,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=300,\n",
    "            out_features=200,\n",
    "        )\n",
    "        self.output = nn.Linear(\n",
    "            in_features=200,\n",
    "            out_features=2,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        _, (last_state, _) = self.bilstm(x)\n",
    "        last_state = last_state.view(x.size(0), -1)\n",
    "        dense_state = nn.functional.relu(self.dense(last_state))\n",
    "        logits = self.output(dense_state)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class STT2Dataset(Dataset):\n",
    "    def __init__(self, sentences, bert_logits):\n",
    "        self.data = sentences\n",
    "        self.labels = bert_logits\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data[idx], self.labels[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor(10.8305, grad_fn=&lt;MseLossBackward&gt;)\ntensor(8.9409, grad_fn=&lt;MseLossBackward&gt;)\ntensor(10.4380, grad_fn=&lt;MseLossBackward&gt;)\ntensor(5.4852, grad_fn=&lt;MseLossBackward&gt;)\ntensor(6.4239, grad_fn=&lt;MseLossBackward&gt;)\ntensor(2.9726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(3.2037, grad_fn=&lt;MseLossBackward&gt;)\ntensor(2.8697, grad_fn=&lt;MseLossBackward&gt;)\ntensor(3.5355, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.3920, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2983, grad_fn=&lt;MseLossBackward&gt;)\ntensor(4.3300, grad_fn=&lt;MseLossBackward&gt;)\ntensor(3.9817, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.7778, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2138, grad_fn=&lt;MseLossBackward&gt;)\ntensor(2.2633, grad_fn=&lt;MseLossBackward&gt;)\ntensor(2.3566, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.3163, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.9506, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4314, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.9735, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.5966, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.3068, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.3159, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5375, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.4089, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.7752, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4062, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.1280, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2249, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.3616, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.7705, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.8750, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1152, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2277, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.8347, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.5432, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0774, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2343, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.7233, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.3672, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1368, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2007, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5836, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2634, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1136, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1660, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5408, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.2230, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1296, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1574, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4813, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1665, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1395, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1268, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4740, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1381, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1237, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0879, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5007, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1121, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1077, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0685, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5301, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0959, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0987, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0686, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5158, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0884, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1040, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0790, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4911, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0805, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1113, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0804, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4862, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0745, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1121, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0804, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4879, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0649, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0773, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.4975, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0560, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1017, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0714, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5131, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0484, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0967, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0687, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5178, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0449, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0977, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0697, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5129, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0443, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0998, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0711, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5094, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0445, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1008, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0720, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5075, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0453, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1016, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0727, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5067, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0454, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1016, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5079, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0449, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1014, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5077, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0448, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1016, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5076, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0446, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1014, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0722, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5087, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0444, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1012, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0721, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5088, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0445, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1014, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5084, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1016, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5081, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0448, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1019, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0727, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5077, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0448, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0727, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5081, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1018, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5090, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0446, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1015, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0723, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5095, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0445, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1015, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0723, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5094, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0446, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1017, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5091, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1019, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5089, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1020, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5091, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1019, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5095, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0446, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1018, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5096, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0446, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1018, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0724, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5096, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0446, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1019, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5095, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1020, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5095, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1020, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5096, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1020, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5097, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1020, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5098, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1020, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5098, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5098, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5098, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5099, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0725, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5099, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1021, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5102, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1023, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5101, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0447, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.1022, grad_fn=&lt;MseLossBackward&gt;)\ntensor(0.0726, grad_fn=&lt;MseLossBackward&gt;)\ntensor(1.5100, grad_fn=&lt;MseLossBackward&gt;)\n"
    }
   ],
   "source": [
    "bilstm_model = BiLSTM()\n",
    "optim = torch.optim.Adam(bilstm_model.parameters())\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_dataset = STT2Dataset(input_representations, bert_logits)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5)\n",
    "\n",
    "for e in range(100):\n",
    "    for batch in train_dataloader:\n",
    "        optim.zero_grad()\n",
    "        x, y = batch\n",
    "        logits = bilstm_model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}